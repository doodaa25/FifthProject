import os
import nltk
import matplotlib.pyplot as plt 
from nltk.tokenize import word_tokenize
from nltk.tag import pos_tag

# --- 1. Setup & Download NLTK Data ---
# We use try/except to avoid re-downloading if you already have them
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt', quiet=True)
    nltk.download('averaged_perceptron_tagger', quiet=True)
    nltk.download('averaged_perceptron_tagger_eng', quiet=True)
    nltk.download('punkt_tab', quiet=True)

# --- 2. Define the Calculation Function ---
def calculate_lexical_density(text):
    """
    Calculates Lexical Density: (Open Class Words / Total Words) * 100
    Open Class Words = Nouns (N), Verbs (V), Adjectives (J), Adverbs (R)
    """
    # Filter for alphabetic words only (Normalization)
    tokens = [word.lower() for word in word_tokenize(text) if word.isalpha()]
    total_words = len(tokens)
    
    if total_words == 0: return 0
    
    # Part-of-Speech Tagging
    tagged_tokens = pos_tag(tokens)
    
    # Filter for Content Words
    content_word_tags = ('N', 'V', 'J', 'R')
    content_words = [word for word, tag in tagged_tokens if tag.startswith(content_word_tags)]
    
    return (len(content_words) / total_words) * 100

# --- 3. Analysis Logic ---
# Assumes this script is running inside the 'notebooks' folder
# and data is in the sibling 'data' folder.
data_folder = "../data"
files_to_analyze = [
    "The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt",
    "The Project Gutenberg eBook of War and Peace, by Leo Tolstoy.txt"
]

results = {} 

print(f"{'Book Name':<60} | {'Density':<10}")
print("-" * 80)

for filename in files_to_analyze:
    file_path = os.path.join(data_folder, filename)
    
    # Fallback: Check current directory if ../data fails
    if not os.path.exists(file_path):
        file_path = filename 

    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()
            
            # Calculate density
            density = calculate_lexical_density(text)
            
            # Clean up the name for the chart
            clean_name = filename.replace("The Project Gutenberg eBook of ", "").replace(".txt", "")
            
            # Store result for plotting
            results[clean_name] = density
            
            print(f"{clean_name:<60} | {density:.2f}%")
            
    except FileNotFoundError:
        print(f"Error: Could not find '{filename}'. Please check your data folder path.")

# --- 4. Generate the Chart ---
if results:
    names = list(results.keys())
    values = list(results.values())

    plt.figure(figsize=(10, 6))
    
    # Create the bars
    bars = plt.bar(names, values, color=['#4CAF50', '#2196F3']) 
    
    plt.ylabel('Lexical Density (%)')
    plt.title('Lexical Density Comparison: Tolstoy\'s Works')
    plt.ylim(0, 60) 

    # Add text numbers on top of bars
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2.0, height, f'{height:.2f}%', ha='center', va='bottom')

    # If running as a script, this pops up a window. 
    # In a notebook, it displays inline.
    print("\nGenerating Chart...")
    plt.show()
