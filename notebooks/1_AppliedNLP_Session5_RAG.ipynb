{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb4daedf",
   "metadata": {},
   "source": [
    "# Session 5 — Retrieval Augmented Generation (RAG)\n",
    "\n",
    "In this notebook, we build a **minimal RAG pipeline** using Lewis Carroll's two Alice books as our corpus as usual:\n",
    "\n",
    "- *Alice's Adventures in Wonderland*\n",
    "- *Through the Looking-Glass*\n",
    "\n",
    "> **IMPORTANT**: It is **highly recommended** to use a virtual environment for this session!  \n",
    "> The packages and downloaded models (embeddings, transformers) can easily reach over **1 GB** in size.  \n",
    "> Using a venv keeps your system clean and makes it easy to manage these large dependencies and delete them when not needed anymore.\n",
    "\n",
    "## What is RAG?\n",
    "\n",
    "**Retrieval-Augmented Generation (RAG)** is a technique that enhances LLM responses by:\n",
    "1. **Retrieving** relevant information from a knowledge base (your documents)\n",
    "2. **Augmenting** the LLM prompt with this retrieved context\n",
    "3. **Generating** an answer based on both the question and the retrieved information\n",
    "\n",
    "This approach allows LLMs to answer questions about documents they weren't trained on, and reduces hallucinations by grounding responses in actual source material.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "We will:\n",
    "\n",
    "1. **Load** the two books as plain text  \n",
    "2. **Split** them into **overlapping chunks** (text segmentation)  \n",
    "3. **Create embeddings** for each chunk (convert text to vectors)  \n",
    "4. **Store** them in a **vector database (FAISS)** for efficient similarity search  \n",
    "5. **Build** a **retrieval + generation chain** to answer questions about the books  \n",
    "6. **Query** the system with natural language questions\n",
    "\n",
    "The focus is on understanding the *pipeline*, not on perfect model choices. You can swap components (embeddings, LLMs, vector stores) as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d83a5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Hub for pulling prompts\n",
    "from langsmith import Client\n",
    "hub = Client()\n",
    "\n",
    "# LLM: we use Ollama (local) here to avoid API keys\n",
    "# Make sure you have installed and started Ollama, and pulled a model, e.g.:\n",
    "#   - install from https://ollama.com\n",
    "#   - in a terminal, run: `ollama pull llama3.2`\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# A small helper for nicer printing\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1220a1c",
   "metadata": {},
   "source": [
    "## Setup & Configuration\n",
    "\n",
    "### LLM Options\n",
    "\n",
    "In this notebook, we use **Ollama** for local LLM inference (no API keys required).\n",
    "\n",
    "**Alternative LLM options:**\n",
    "- **OpenAI**: `from langchain_openai import ChatOpenAI` → requires API key\n",
    "- **Groq**: `from langchain_groq import ChatGroq` → requires API key  \n",
    "- **Anthropic**: `from langchain_anthropic import ChatAnthropic` → requires API key\n",
    "- **HuggingFace**: `from langchain_huggingface import HuggingFaceEndpoint` → requires API key\n",
    "\n",
    "**To use Ollama:**\n",
    "1. Install from [https://ollama.com/download](https://ollama.com/download)\n",
    "2. Run in terminal: `ollama pull llama3.2` (or another model)\n",
    "3. Ollama runs on `localhost:11434` by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13b93f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the Alice books (plain text)\n",
    "# Adjust these paths if your files live somewhere else.\n",
    "DATA_DIR = Path(\"../data\")\n",
    "WARANDPEACE_PATH = DATA_DIR / \"The Project Gutenberg eBook of War and Peace, by Leo Tolstoy.txt\"\n",
    "ANNAKARENINA_PATH = DATA_DIR / \"The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt\"\n",
    "\n",
    "# Set up local LLM via Ollama\n",
    "# If you prefer Groq or OpenAI, you can swap this block for your own client.\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    temperature=0.0  # Controls randomness: 0.0 = deterministic, 1.0 = creative\n",
    ")\n",
    "\n",
    "# print(\"Data directory:\", DATA_DIR.resolve())\n",
    "# print(\"Using LLM model:\", \"llama3.2 (Ollama)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00ff56",
   "metadata": {},
   "source": [
    "### Configuration Notes\n",
    "\n",
    "**Model Selection:**\n",
    "- `llama3.2`: Fast, good for local testing (3B parameters)\n",
    "- Other Ollama models: `llama3.1`, `mistral`, `phi3` (run `ollama list` to see installed models)\n",
    "\n",
    "**Temperature Setting:**\n",
    "- `temperature=0.0`: Deterministic responses (same answer every time)\n",
    "- `temperature=0.7`: More creative/varied responses\n",
    "- `temperature=1.0`: Maximum creativity (may be less factual)\n",
    "\n",
    "For RAG applications, **lower temperatures (0.0-0.3)** are recommended to keep answers focused on retrieved content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e838ebd9",
   "metadata": {},
   "source": [
    "## 1. Load books\n",
    "\n",
    "We reuse the idea of the **`load_book`** helper from earlier sessions, but keep it simple:\n",
    "\n",
    "**Steps:**\n",
    "1. **Read** the text file from disk\n",
    "2. **Strip** Project Gutenberg header/footer (boilerplate text)\n",
    "3. **Return** clean text ready for processing\n",
    "\n",
    "**Why clean the text?**\n",
    "- Project Gutenberg files contain legal notices and metadata\n",
    "- These sections aren't part of the actual book content\n",
    "- Including them would pollute our embeddings with irrelevant information\n",
    "\n",
    "**Data Sources:**\n",
    "- You can use any plain text files (`.txt`)\n",
    "- For other formats: PDF → use `PyPDF2` or `pdfplumber`, DOCX → use `python-docx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "498325ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "War and Peace: 3,203,378 characters after cleaning\n",
      "Anna Karenina: 1,963,519 characters after cleaning\n"
     ]
    }
   ],
   "source": [
    "def load_book(filepath: Path, name: str) -> str:\n",
    "    # Load and roughly clean a Project Gutenberg text file.\n",
    "    if not filepath.exists():\n",
    "        raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Very simple cleaning: try to cut away Gutenberg boilerplate\n",
    "    start_markers = [\"CHAPTER I\", \"*** START OF\"]\n",
    "    end_markers = [\"*** END OF\", \"End of Project Gutenberg\"]\n",
    "\n",
    "    start_idx = 0\n",
    "    for marker in start_markers:\n",
    "        if marker in text:\n",
    "            start_idx = text.find(marker)\n",
    "            break\n",
    "\n",
    "    end_idx = len(text)\n",
    "    for marker in end_markers:\n",
    "        if marker in text:\n",
    "            end_idx = text.find(marker)\n",
    "            break\n",
    "\n",
    "    cleaned = text[start_idx:end_idx].strip()\n",
    "    print(f\"{name}: {len(cleaned):,} characters after cleaning\")\n",
    "    return cleaned\n",
    "\n",
    "warandpeace_text = load_book(WARANDPEACE_PATH, \"War and Peace\")\n",
    "annakarenina_text = load_book(ANNAKARENINA_PATH, \"Anna Karenina\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cc3da0",
   "metadata": {},
   "source": [
    "## 2. Chunk the texts for retrieval\n",
    "\n",
    "Large documents are **too long** to embed and retrieve as a single vector.  \n",
    "Instead, we split the books into **overlapping chunks**:\n",
    "\n",
    "### Parameters Explained\n",
    "\n",
    "- **`chunk_size`**: Maximum number of characters per chunk (default: 800)\n",
    "  - This is a **hard limit** - chunks won't exceed this size\n",
    "  - Too small → loses context, more chunks to search\n",
    "  - Too large → less precise retrieval, may exceed embedding model limits\n",
    "  - **Typical range**: 500-1500 characters\n",
    "\n",
    "- **`chunk_overlap`**: How much neighboring chunks overlap (default: 150)\n",
    "  - Ensures sentences near boundaries aren't split awkwardly\n",
    "  - Helps maintain context across chunk boundaries\n",
    "  - **Typical range**: 10-20% of chunk_size\n",
    "\n",
    "- **`separators`**: Priority order for splitting points\n",
    "  - These determine **where** to split when approaching the chunk_size limit\n",
    "  - The splitter tries each separator in order to find a natural break point:\n",
    "    1. `\\n\\n` → paragraph breaks (preferred - most context preserved)\n",
    "    2. `\\n` → line breaks\n",
    "    3. `. ` → sentence endings\n",
    "    4. ` ` → word boundaries (last resort)\n",
    "  - **Key point**: The splitter builds chunks up to ~800 chars, then looks for the best separator to split on\n",
    "\n",
    "### How It Works Together\n",
    "\n",
    "Example: If text reaches 780 characters, the splitter looks for the first `\\n\\n` (paragraph break). If found, it splits there (even if only 750 chars). If not found, it tries `\\n`, then `. `, then ` `. This keeps chunks **under 800 chars** while breaking at **natural boundaries**.\n",
    "\n",
    "### Experimentation\n",
    "\n",
    "Try adjusting these values to see how they affect:\n",
    "- Number of chunks created\n",
    "- Retrieval quality\n",
    "- Answer accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b86b7254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "War and Peace: 5711 chunks (chunk_size=800, overlap=150)\n",
      "Anna Karenina: 3517 chunks (chunk_size=800, overlap=150)\n",
      "Total chunks in corpus: 9228\n"
     ]
    }
   ],
   "source": [
    "def chunk_text(text: str, book_name: str, chunk_size: int = 800, chunk_overlap: int = 150):\n",
    "    # Split a long text into overlapping chunks using RecursiveCharacterTextSplitter.\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"],  # try to keep chunks on sentence/paragraph boundaries\n",
    "    )\n",
    "    docs = splitter.create_documents([text])\n",
    "    print(f\"{book_name}: {len(docs)} chunks (chunk_size={chunk_size}, overlap={chunk_overlap})\")\n",
    "    return docs\n",
    "\n",
    "warandpeace_chunks = chunk_text(warandpeace_text, \"War and Peace\")\n",
    "annakarenina_chunks = chunk_text(annakarenina_text, \"Anna Karenina\")\n",
    "\n",
    "# Combine chunks from both books into a single corpus\n",
    "all_chunks = warandpeace_chunks + annakarenina_chunks\n",
    "print(\"Total chunks in corpus:\", len(all_chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fc3f1d",
   "metadata": {},
   "source": [
    "## 3. Create embeddings & build a vector database (FAISS)\n",
    "\n",
    "### What are embeddings? (We've been using them since session 3)\n",
    "\n",
    "**Embeddings** convert text into numerical vectors (arrays of numbers) that capture semantic meaning:\n",
    "- Similar texts → similar vectors\n",
    "- Enables mathematical similarity comparisons\n",
    "- Typical dimensions: 384, 768, or 1536 numbers per chunk\n",
    "\n",
    "### Vector Database (FAISS)\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) is a library for efficient similarity search:\n",
    "- Stores all chunk embeddings\n",
    "- Quickly finds the most similar chunks to a query\n",
    "- Works entirely offline (no API needed)\n",
    "\n",
    "### Embedding Model Options\n",
    "\n",
    "**Current**: `sentence-transformers/all-mpnet-base-v2`\n",
    "- Dimensions: 768\n",
    "- Quality: High for general-purpose tasks\n",
    "- Speed: Medium\n",
    "\n",
    "**Alternatives:**\n",
    "- `all-MiniLM-L6-v2` → Faster, smaller (384 dim), slightly lower quality (you've already used this one)\n",
    "- `all-mpnet-base-v1` → Similar to v2\n",
    "- OpenAI embeddings → `text-embedding-3-small` (requires API key)\n",
    "\n",
    "**To change**: Just replace the `model_name` parameter in `HuggingFaceEmbeddings()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04cd13e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0631b83824b42519196be98e908b905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Omen\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "146b02d948a544358ec7216b5655a160",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6f3be1de24f45feb79ed202738c55f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d39fc694f2c43debe9e67b8961094aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44cad3a4b1dc4644bdac76fd452e893f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58346f3a787746d08b2387e2aa3481ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cfad00ba0d54405a1539baad73ec50d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39991ab839c04786aedfab8538593d40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27a65a9bdeec4e0289e8dfae2ef45d28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6cf209668064165a5501d89b3a392e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e8c4d97f10d4e85a9b2f653bf4d696d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     vectorstore.save_local(\u001b[38;5;28mstr\u001b[39m(db_path))\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVector database saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mcreate_embedding_vector_db\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_chunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mVECTOR_DB_PATH\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mcreate_embedding_vector_db\u001b[39m\u001b[34m(chunks, db_path)\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_embedding_vector_db\u001b[39m(chunks, db_path: Path):\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# 1. Instantiate an embedding model (HuggingFace embeddings)\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;66;03m# 2. Create a FAISS vector store from the chunks\u001b[39;00m\n\u001b[32m      8\u001b[39m     \u001b[38;5;66;03m# 3. Save it locally so we can reload it later\u001b[39;00m\n\u001b[32m      9\u001b[39m     embedding = HuggingFaceEmbeddings(\n\u001b[32m     10\u001b[39m         model_name=\u001b[33m\"\u001b[39m\u001b[33msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     vectorstore = \u001b[43mFAISS\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m        \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43membedding\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     vectorstore.save_local(\u001b[38;5;28mstr\u001b[39m(db_path))\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVector database saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:807\u001b[39m, in \u001b[36mVectorStore.from_documents\u001b[39m\u001b[34m(cls, documents, embedding, **kwargs)\u001b[39m\n\u001b[32m    804\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ids):\n\u001b[32m    805\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mids\u001b[39m\u001b[33m\"\u001b[39m] = ids\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001b[39m, in \u001b[36mFAISS.from_texts\u001b[39m\u001b[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001b[39m\n\u001b[32m   1016\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   1017\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfrom_texts\u001b[39m(\n\u001b[32m   1018\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1023\u001b[39m     **kwargs: Any,\n\u001b[32m   1024\u001b[39m ) -> FAISS:\n\u001b[32m   1025\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001b[39;00m\n\u001b[32m   1026\u001b[39m \n\u001b[32m   1027\u001b[39m \u001b[33;03m    This is a user friendly interface that:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1041\u001b[39m \u001b[33;03m            faiss = FAISS.from_texts(texts, embeddings)\u001b[39;00m\n\u001b[32m   1042\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1043\u001b[39m     embeddings = \u001b[43membedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1044\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.__from(\n\u001b[32m   1045\u001b[39m         texts,\n\u001b[32m   1046\u001b[39m         embeddings,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1050\u001b[39m         **kwargs,\n\u001b[32m   1051\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:155\u001b[39m, in \u001b[36mHuggingFaceEmbeddings.embed_documents\u001b[39m\u001b[34m(self, texts)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34membed_documents\u001b[39m(\u001b[38;5;28mself\u001b[39m, texts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[32m    146\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Compute doc embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m    148\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    153\u001b[39m \n\u001b[32m    154\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\langchain_huggingface\\embeddings\\huggingface.py:130\u001b[39m, in \u001b[36mHuggingFaceEmbeddings._embed\u001b[39m\u001b[34m(self, texts, encode_kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m     sentence_transformers.SentenceTransformer.stop_multi_process_pool(pool)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    132\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mencode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(embeddings, \u001b[38;5;28mlist\u001b[39m):\n\u001b[32m    137\u001b[39m     msg = (\n\u001b[32m    138\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mExpected embeddings to be a Tensor or a numpy array, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mgot a list instead.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    140\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1094\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1091\u001b[39m features.update(extra_features)\n\u001b[32m   1093\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m     out_features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.device.type == \u001b[33m\"\u001b[39m\u001b[33mhpu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   1096\u001b[39m         out_features = copy.deepcopy(out_features)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1175\u001b[39m, in \u001b[36mSentenceTransformer.forward\u001b[39m\u001b[34m(self, input, **kwargs)\u001b[39m\n\u001b[32m   1169\u001b[39m             module_kwarg_keys = \u001b[38;5;28mself\u001b[39m.module_kwargs.get(module_name, [])\n\u001b[32m   1170\u001b[39m         module_kwargs = {\n\u001b[32m   1171\u001b[39m             key: value\n\u001b[32m   1172\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m kwargs.items()\n\u001b[32m   1173\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module_kwarg_keys \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(module, \u001b[33m\"\u001b[39m\u001b[33mforward_kwargs\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module.forward_kwargs)\n\u001b[32m   1174\u001b[39m         }\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\sentence_transformers\\models\\Transformer.py:261\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, features, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    239\u001b[39m \u001b[33;03mForward pass through the transformer model.\u001b[39;00m\n\u001b[32m    240\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    257\u001b[39m \u001b[33;03m        - 'all_layer_embeddings': If the model outputs hidden states, contains embeddings from all layers\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    259\u001b[39m trans_features = {key: value \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m features.items() \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model_forward_params}\n\u001b[32m--> \u001b[39m\u001b[32m261\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mauto_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtrans_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m token_embeddings = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    263\u001b[39m features[\u001b[33m\"\u001b[39m\u001b[33mtoken_embeddings\u001b[39m\u001b[33m\"\u001b[39m] = token_embeddings\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:486\u001b[39m, in \u001b[36mMPNetModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    484\u001b[39m head_mask = \u001b[38;5;28mself\u001b[39m.get_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m.config.num_hidden_layers)\n\u001b[32m    485\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(input_ids=input_ids, position_ids=position_ids, inputs_embeds=inputs_embeds)\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m sequence_output = encoder_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    495\u001b[39m pooled_output = \u001b[38;5;28mself\u001b[39m.pooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.pooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:338\u001b[39m, in \u001b[36mMPNetEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[39m\n\u001b[32m    335\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    336\u001b[39m     all_hidden_states = all_hidden_states + (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    340\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    342\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    343\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    344\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    346\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    348\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:308\u001b[39m, in \u001b[36mMPNetLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, head_mask, position_bias, output_attentions, **kwargs)\u001b[39m\n\u001b[32m    305\u001b[39m outputs = self_attention_outputs[\u001b[32m1\u001b[39m:]  \u001b[38;5;66;03m# add self attentions if we output attention weights\u001b[39;00m\n\u001b[32m    307\u001b[39m intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m--> \u001b[39m\u001b[32m308\u001b[39m layer_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    309\u001b[39m outputs = (layer_output,) + outputs\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\transformers\\models\\mpnet\\modeling_mpnet.py:275\u001b[39m, in \u001b[36mMPNetOutput.forward\u001b[39m\u001b[34m(self, hidden_states, input_tensor)\u001b[39m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     hidden_states = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.dropout(hidden_states)\n\u001b[32m    277\u001b[39m     hidden_states = \u001b[38;5;28mself\u001b[39m.LayerNorm(hidden_states + input_tensor)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Omen\\Documents\\GitHub\\applied-NLP-week5\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "VECTOR_DB_DIR = Path(\"../vector_databases\")\n",
    "VECTOR_DB_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VECTOR_DB_PATH = VECTOR_DB_DIR / \"vector_db_lev\"\n",
    "\n",
    "def create_embedding_vector_db(chunks, db_path: Path):\n",
    "    # 1. Instantiate an embedding model (HuggingFace embeddings)\n",
    "    # 2. Create a FAISS vector store from the chunks\n",
    "    # 3. Save it locally so we can reload it later\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embedding\n",
    "    )\n",
    "\n",
    "    vectorstore.save_local(str(db_path))\n",
    "    print(f\"Vector database saved to: {db_path}\")\n",
    "\n",
    "create_embedding_vector_db(all_chunks, VECTOR_DB_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93044d97",
   "metadata": {},
   "source": [
    "### Performance Notes\n",
    "\n",
    "**First run:**\n",
    "- Downloads the embedding model (~420MB for all-mpnet-base-v2)\n",
    "- Creates embeddings for all chunks (may take 1-2 minutes)\n",
    "- Saves the vector database to disk\n",
    "\n",
    "**Subsequent runs:**\n",
    "- Model is cached locally\n",
    "- Can skip this step if vector database already exists\n",
    "- Just load the saved database (next section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a98fc2",
   "metadata": {},
   "source": [
    "## 4. Build a retriever from the vector database\n",
    "\n",
    "To use RAG, we need a **retriever** object that:\n",
    "\n",
    "1. Takes a user question  \n",
    "2. Converts it to an embedding (using the same model as the chunks)\n",
    "3. Finds the **k most similar chunks** in the vector store using cosine similarity\n",
    "4. Returns those chunks to be passed to the LLM\n",
    "\n",
    "### The `k` Parameter\n",
    "\n",
    "**`k=4`** means \"retrieve the 4 most similar chunks\"\n",
    "\n",
    "**Trade-offs:**\n",
    "- **Low k (1-3)**: Faster, more focused, but might miss relevant information\n",
    "- **Medium k (4-6)**: Balanced approach (recommended starting point)\n",
    "- **High k (7-15)**: More comprehensive, but may include irrelevant chunks and slow down the LLM\n",
    "\n",
    "**Experiment:** Try different `k` values to see how they affect answer quality and response time.\n",
    "\n",
    "### Search Strategies\n",
    "\n",
    "FAISS supports different search algorithms:\n",
    "- **Similarity search** (default): Returns top-k most similar chunks (we use this one here on this notebook)\n",
    "- **MMR** (Maximum Marginal Relevance): Returns diverse results\n",
    "- **Similarity with score threshold**: Only returns chunks above a certain similarity score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fcbe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_retriever(db_path: Path, k: int = 4):\n",
    "    # Reload the FAISS vector store from disk and create a retriever.\n",
    "    embedding = HuggingFaceEmbeddings(\n",
    "        model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    "    )\n",
    "\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=str(db_path),\n",
    "        embeddings=embedding,\n",
    "        allow_dangerous_deserialization=True,  # needed in some environments\n",
    "    )\n",
    "\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    print(f\"Retriever ready (k={k}) from {db_path}\")\n",
    "    return retriever\n",
    "\n",
    "lev_retriever = load_retriever(VECTOR_DB_PATH, k=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c0fc6f",
   "metadata": {},
   "source": [
    "## 5. Connect retriever + LLM = RAG chain\n",
    "\n",
    "We now create a **retrieval chain** using **LCEL** (LangChain Expression Language):\n",
    "\n",
    "### Pipeline Flow\n",
    "\n",
    "1. **Input** → User's question\n",
    "2. **Retriever** → Fetches relevant chunks from vector database\n",
    "3. **Format** → Combines chunks into context string\n",
    "4. **Prompt** → Creates LLM prompt with context + question\n",
    "5. **LLM** → Generates answer based on context\n",
    "6. **Output Parser** → Extracts clean string from LLM response\n",
    "\n",
    "### Custom Prompt Design\n",
    "\n",
    "Our prompt instructs the LLM to:\n",
    "- Use only the provided context (retrieved chunks)\n",
    "- **Cite specific passages** from the books\n",
    "- Include brief quotes to support answers\n",
    "- Avoid making up information not in the context\n",
    "\n",
    "### Prompt Customization Options\n",
    "\n",
    "You can modify the system message to change LLM behavior:\n",
    "- Add stricter citation requirements\n",
    "- Request different answer formats (bullet points, summaries, etc.)\n",
    "- Specify answer length constraints\n",
    "- Add domain-specific instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d118fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rag_chain(retriever):\n",
    "    # Connects the retriever with an LLM using a custom prompt that asks for references.\n",
    "    \n",
    "    # Custom prompt that instructs the LLM to cite sources\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"\"\"You are a helpful assistant answering questions about Lewis Carroll's Alice books.\n",
    "Use the following context to answer the question. Always cite specific passages from the books in your answer.\n",
    "When you use information from the context, include a brief quote or reference to show where it came from.\n",
    "\n",
    "Context:\n",
    "{context}\"\"\"),\n",
    "        (\"human\", \"{input}\")\n",
    "    ])\n",
    "    \n",
    "    def format_docs(docs):\n",
    "        return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "    \n",
    "    # Build RAG chain using LCEL (LangChain Expression Language)\n",
    "    # This dictionary creates two inputs for the prompt:\n",
    "    # - \"context\": runs retriever, gets docs, formats them as string → fills {context} placeholder\n",
    "    # - \"input\": passes user's question through unchanged → fills {input} placeholder\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"input\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    return rag_chain\n",
    "\n",
    "lev_rag_chain = build_rag_chain(lev_retriever)\n",
    "print(\"RAG chain ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aeef065",
   "metadata": {},
   "source": [
    "### Alternative Prompt Strategies\n",
    "\n",
    "**Without citations** (original hub prompt):\n",
    "```python\n",
    "prompt = hub.pull_prompt(\"langchain-ai/retrieval-qa-chat\")\n",
    "```\n",
    "\n",
    "**With structured output:**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer in this format:\n",
    "    ANSWER: [your answer]\n",
    "    SOURCES: [relevant quotes]\n",
    "    \"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```\n",
    "\n",
    "**With confidence levels:**\n",
    "```python\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"Answer the question and rate your confidence (low/medium/high) \n",
    "    based on how well the context supports your answer.\"\"\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1140543",
   "metadata": {},
   "source": [
    "## 6. Ask questions about our books\n",
    "\n",
    "Now we can **chat with the corpus**!\n",
    "\n",
    "### How It Works\n",
    "\n",
    "When you ask a question:\n",
    "1. Question → embedding vector\n",
    "2. Vector database → finds 4 most similar chunks\n",
    "3. Chunks + question → sent to LLM as context\n",
    "4. LLM → generates answer with citations\n",
    "5. Answer → displayed with text wrapping\n",
    "\n",
    "### Key Points\n",
    "\n",
    "- The LLM **does NOT answer from its pretraining alone**\n",
    "- It first retrieves relevant chunks from the *Alice* books\n",
    "- Answers are **grounded in the actual text**\n",
    "- Citations help verify the information\n",
    "\n",
    "### Evaluation Tips\n",
    "\n",
    "When testing your RAG system, consider:\n",
    "- **Relevance**: Does the answer address the question?\n",
    "- **Accuracy**: Is the information correct per the source?\n",
    "- **Citation quality**: Are quotes/references provided?\n",
    "- **Completeness**: Does it cover all relevant aspects?\n",
    "- **No hallucination**: Does it avoid making up information?\n",
    "\n",
    "Try questions that:\n",
    "- Require specific details (names, events)\n",
    "- Need synthesis across multiple passages\n",
    "- Ask about comparisons between the books\n",
    "- Test the system's limits (questions not answerable from the text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e939af60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_lev(question: str, chain=lev_rag_chain):\n",
    "    # Send a question to the RAG chain and print a nicely wrapped answer.\n",
    "    print(f\"\\nQUESTION:\\n{question}\\n\" + \"-\"*80)\n",
    "    answer = chain.invoke(question)\n",
    "    print(\"\\nANSWER:\\n\")\n",
    "    print(textwrap.fill(answer, width=100))\n",
    "\n",
    "# Example questions\n",
    "ask_lev(\"Can you give me sentences of when it's critical point for Levin's character development?\")\n",
    "ask_lev(\"What happened in the end of war in War and Peace?\")\n",
    "ask_lev(\"How is Kitty's character portrayed throughout the novel?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1419194f",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [LangChain Documentation](https://python.langchain.com/)\n",
    "- [FAISS Documentation](https://faiss.ai/)\n",
    "- [Sentence Transformers](https://www.sbert.net/)\n",
    "- [Ollama Models](https://ollama.com/library)\n",
    "- [RAG Survey Paper](https://arxiv.org/abs/2312.10997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75442753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
