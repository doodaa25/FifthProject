import json
import os

# Define the path where the notebook will be created
# Based on your screenshot, this should be in the 'notebooks' directory
output_dir = "notebooks"
output_filename = "2_lexical_density.ipynb"
output_path = os.path.join(output_dir, output_filename)

# Ensure directory exists
os.makedirs(output_dir, exist_ok=True)

# The content of the notebook
notebook_content = {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexical Density Analysis\n",
    "\n",
    "This notebook analyzes the lexical density of 'Anna Karenina' and 'War and Peace'.\n",
    "\n",
    "**Formula:** $Lexical Density = \\frac{Total Content Words}{Total Words} \\times 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": None,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# Download necessary NLTK data (only needs to be done once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "def calculate_lexical_density(text):\n",
    "    \"\"\"\n",
    "    Calculates lexical density: (Content Words / Total Words) * 100\n",
    "    Content Words = Nouns, Verbs, Adjectives, Adverbs\n",
    "    \"\"\"\n",
    "    # Tokenize and filter out non-alphabetic tokens (punctuation, numbers)\n",
    "    tokens = [word.lower() for word in word_tokenize(text) if word.isalpha()]\n",
    "    \n",
    "    total_words = len(tokens)\n",
    "    if total_words == 0:\n",
    "        return 0\n",
    "    \n",
    "    # Tag parts of speech\n",
    "    # NN: Noun, VB: Verb, JJ: Adjective, RB: Adverb\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    content_word_tags = ('N', 'V', 'J', 'R')\n",
    "    content_words = [word for word, tag in tagged_tokens if tag.startswith(content_word_tags)]\n",
    "    \n",
    "    density = (len(content_words) / total_words) * 100\n",
    "    return density, total_words, len(content_words)\n",
    "\n",
    "# Define paths relative to the 'notebooks' folder\n",
    "data_folder = \"../data\"\n",
    "files_to_analyze = [\n",
    "    \"The Project Gutenberg eBook of Anna Karenina, by Leo Tolstoy.txt\",\n",
    "    \"The Project Gutenberg eBook of War and Peace, by Leo Tolstoy.txt\"\n",
    "]\n",
    "\n",
    "print(f\"{'Book Name':<60} | {'Lexical Density':<15} | {'Total Words'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for filename in files_to_analyze:\n",
    "    file_path = os.path.join(data_folder, filename)\n",
    "    \n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "            # Calculate density\n",
    "            density, total, content = calculate_lexical_density(text)\n",
    "            \n",
    "            # Clean up filename for display\n",
    "            display_name = filename.replace(\"The Project Gutenberg eBook of \", \"\").replace(\".txt\", \"\")\n",
    "            \n",
    "            print(f\"{display_name:<60} | {density:.2f}%          | {total:,}\")\n",
    "            \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find file at {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

# Write the file
with open(output_path, "w", encoding="utf-8") as f:
    json.dump(notebook_content, f, indent=1)

print(f"Success! '{output_path}' has been created.")